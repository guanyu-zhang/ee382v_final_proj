{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os, shutil, re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.metrics import classification_report "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bivar_transform(value, standard):\n",
    "    \"\"\"\n",
    "    parms:\n",
    "    value: input label\n",
    "    standard: the label that is preserved, all other labels are set to be 'other'\n",
    "    standrd is in {'1', '2', 'b', 'c', 'm', 'p', 't', 'x'}\n",
    "    \"\"\"\n",
    "    if value != standard:\n",
    "        return 'other'\n",
    "    else:\n",
    "        return standard\n",
    "    \n",
    "def bivar(data, standard):\n",
    "    data_cp = data.copy()\n",
    "    for i in range(data.shape[0]):\n",
    "        temp = bivar_transform(data.iloc[:,-1][i], standard)\n",
    "        data_cp.iloc[:,-1][i] = temp\n",
    "    return data_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_path = os.getcwd()\n",
    "dir_list = os.listdir(os.path.join(main_path,'p1'))\n",
    "def lopo(p, standard):\n",
    "    \"\"\"\n",
    "    LOPO\n",
    "    parms:\n",
    "    p: leave one participant out p in [1,2,3,4,5,6,7]\n",
    "    return: two dictionaries (train, test)\n",
    "    \"\"\"\n",
    "    train_data = {}\n",
    "    test_data = {}\n",
    "    main_list = ['p'+str(i) for i in range(1,8)]\n",
    "    main_list.remove('p'+str(p))\n",
    "    for i in main_list:\n",
    "        for j in dir_list:\n",
    "            d = pd.read_csv(os.path.join(main_path,i,j),low_memory=False)\n",
    "            d = bivar(d, standard)\n",
    "            d = np.array(d)\n",
    "            if train_data.get(j) is None:\n",
    "                train_data[j] = d\n",
    "            else:\n",
    "                train_data[j] = np.r_[train_data[j],d]\n",
    "    \n",
    "    for i in dir_list:\n",
    "        d = pd.read_csv(os.path.join(main_path,'p'+str(p),i),low_memory=False)\n",
    "        d = bivar(d, standard)\n",
    "        d = np.array(d)\n",
    "        test_data[i] = d\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lopo_count(data, standard):\n",
    "    \"\"\"\n",
    "    parms:\n",
    "    data: could be one of train_data and test_data\n",
    "    standard: same as bivar_transform\n",
    "    return: tuple contains the number of two labels (standard and 'other')\n",
    "    \"\"\"\n",
    "    standard_count, other_count = 0, 0\n",
    "    for i in data:\n",
    "        count = Counter(data[i][:,-1])\n",
    "        if len(count) == 2:\n",
    "            other_count += count['other']\n",
    "            standard_count += count[standard]\n",
    "        else:\n",
    "            other_count += count['other']\n",
    "    return standard_count, other_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-12-be0bada80baa>:17: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  data_cp.iloc[:,-1][i] = temp\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train_data: 1111 'm' 34148 'other' \n",
      "In test_data: 91 'm' 6581 'other' \n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "这里用 'm' 和 p7 为例\n",
    "\"\"\"\n",
    "train_data, test_data = lopo(p=7, standard='m')\n",
    "m_count_train, other_count_train = lopo_count(train_data, standard='m')\n",
    "m_count_test, other_count_test = lopo_count(test_data, standard='m')\n",
    "print('In train_data: ' + str(m_count_train) + \" 'm' \" + str(other_count_train) + \" 'other' \")\n",
    "print('In test_data: ' + str(m_count_test) + \" 'm' \" + str(other_count_test) + \" 'other' \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "对每个场景单独训练模型一共6个,这里我用了rf和adaboost\n",
    "\"\"\"\n",
    "def model_dict(data, model: str):\n",
    "    \"\"\"\n",
    "    parms:\n",
    "    model: model in ['rf','adaboost']\n",
    "    \"\"\"\n",
    "    model_dict = dict()\n",
    "    if model == 'rf':\n",
    "        for i in data:\n",
    "            rfc = RandomForestClassifier(max_depth=30, random_state=0) #max_depth ~ sqrt(n_features=1000)\n",
    "            rfc.fit(data[i][:,:-1],data[i][:,-1])\n",
    "            model_dict[i] = rfc\n",
    "    elif model == 'adaboost':\n",
    "        for i in data:\n",
    "            ada = AdaBoostClassifier(n_estimators=50, random_state=0)\n",
    "            ada.fit(data[i][:,:-1],data[i][:,-1])\n",
    "            model_dict[i] = ada\n",
    "    return model_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dict_rf = model_dict(train_data, model='rf')\n",
    "model_dict_ada = model_dict(train_data, model='adaboost')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "score for outdoor.csv\n",
      "rf 0.9576023391812866\n",
      "adaboost 0.9298245614035088\n",
      "\n",
      "\n",
      "score for reading.csv\n",
      "rf 1.0\n",
      "adaboost 1.0\n",
      "\n",
      "\n",
      "score for call.csv\n",
      "rf 0.9546351084812623\n",
      "adaboost 0.9349112426035503\n",
      "\n",
      "\n",
      "score for dinner.csv\n",
      "rf 0.9943342776203966\n",
      "adaboost 0.9902423670129052\n",
      "\n",
      "\n",
      "score for game.csv\n",
      "rf 0.9586206896551724\n",
      "adaboost 0.9482758620689655\n",
      "\n",
      "\n",
      "score for TV.csv\n",
      "rf 1.0\n",
      "adaboost 1.0\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "rf/adaboost as the baseline and calculate the score for test_data\n",
    "\"\"\"\n",
    "for i in test_data:\n",
    "    print('score for ' + i)\n",
    "    print('rf',model_dict_rf[i].score(test_data[i][:,:-1],test_data[i][:,-1]))\n",
    "    print('adaboost',model_dict_ada[i].score(test_data[i][:,:-1],test_data[i][:,-1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n强烈建议试试其他人或者其他标签\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "强烈建议试试其他人或者其他标签\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这里以下到下一个注释是之前写的有点乱。。。\n",
    "表示的是不做二分类，强行用rf/adaboost对六个场景所有标签直接分类. 效果很差\n",
    "\"\"\"\n",
    "main_path = os.getcwd()\n",
    "dir_list = os.listdir(os.path.join(main_path,'p1'))\n",
    "original_data = {}\n",
    "main_list = ['p'+str(i) for i in range(1,7)]\n",
    "for i in main_list:\n",
    "    for j in dir_list:\n",
    "        d = pd.read_csv(os.path.join(main_path,i,j),low_memory=False)\n",
    "        d = np.array(d)\n",
    "        if original_data.get(j) is None:\n",
    "            original_data[j] = d\n",
    "        else:\n",
    "            original_data[j] = np.r_[original_data[j],d]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_test_data = dict()\n",
    "for i in dir_list:\n",
    "    d = pd.read_csv(os.path.join(main_path,'p7',i),low_memory=False)\n",
    "    d = np.array(d)\n",
    "    original_test_data[i] = d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_model_dict = dict()\n",
    "for i in data:\n",
    "    rfc = RandomForestClassifier(max_depth=30, random_state=0)\n",
    "    rfc.fit(original_data[i][:,:-1],original_data[i][:,-1])\n",
    "    original_model_dict[i] = rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " outdoor.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.74      0.72       273\n",
      "           2       0.28      0.31      0.29       100\n",
      "           b       0.86      0.87      0.86       284\n",
      "           m       0.00      0.00      0.00        26\n",
      "           x       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.70       684\n",
      "   macro avg       0.37      0.38      0.38       684\n",
      "weighted avg       0.68      0.70      0.69       684\n",
      "\n",
      "\n",
      " reading.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.97      0.99      0.98       213\n",
      "           b       0.86      0.67      0.75        18\n",
      "\n",
      "    accuracy                           0.97       231\n",
      "   macro avg       0.91      0.83      0.87       231\n",
      "weighted avg       0.96      0.97      0.96       231\n",
      "\n",
      "\n",
      " call.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.91      0.98      0.94       336\n",
      "           b       0.00      0.00      0.00        23\n",
      "           m       0.00      0.00      0.00        23\n",
      "           p       0.80      0.94      0.86       125\n",
      "\n",
      "    accuracy                           0.88       507\n",
      "   macro avg       0.43      0.48      0.45       507\n",
      "weighted avg       0.80      0.88      0.84       507\n",
      "\n",
      "\n",
      " dinner.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.93      0.69      0.79       557\n",
      "           2       0.39      0.38      0.39       198\n",
      "           b       0.70      0.94      0.81      1462\n",
      "           m       0.00      0.00      0.00        18\n",
      "           p       0.00      0.00      0.00         7\n",
      "           t       0.73      0.48      0.58       922\n",
      "           x       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.72      3177\n",
      "   macro avg       0.39      0.36      0.37      3177\n",
      "weighted avg       0.72      0.72      0.70      3177\n",
      "\n",
      "\n",
      " game.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.82      0.77       201\n",
      "           2       0.59      0.57      0.58        72\n",
      "           b       0.90      0.91      0.90       278\n",
      "           m       0.00      0.00      0.00        24\n",
      "           t       0.00      0.00      0.00         0\n",
      "           x       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.79       580\n",
      "   macro avg       0.37      0.38      0.37       580\n",
      "weighted avg       0.75      0.79      0.77       580\n",
      "\n",
      "\n",
      " TV.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       1.00      0.23      0.37        35\n",
      "           2       0.00      0.00      0.00         0\n",
      "           b       0.38      0.10      0.16        30\n",
      "           t       0.97      1.00      0.98      1428\n",
      "\n",
      "    accuracy                           0.96      1493\n",
      "   macro avg       0.59      0.33      0.38      1493\n",
      "weighted avg       0.96      0.96      0.95      1493\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in original_test_data:\n",
    "    temp_data = original_test_data[i]\n",
    "    print(\"\\n\",i)\n",
    "    y_pred = original_model_dict[i].predict(temp_data[:,:-1]) \n",
    "    print(classification_report(original_test_data[i][:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "outdoor.csv\n",
      "Counter({'b': 791, '1': 768, '2': 746, 'm': 179, 'x': 125, 'c': 82})\n",
      "Counter({'b': 284, '1': 273, '2': 100, 'm': 26, 'x': 1})\n",
      "\n",
      "\n",
      "reading.csv\n",
      "Counter({'1': 1084, 'b': 138, 'x': 5, '2': 1, 'm': 1})\n",
      "Counter({'1': 213, 'b': 18})\n",
      "\n",
      "\n",
      "call.csv\n",
      "Counter({'p': 1112, '1': 973, 'b': 142, 'm': 129, 'c': 13, '2': 5})\n",
      "Counter({'1': 336, 'p': 125, 'b': 23, 'm': 23})\n",
      "\n",
      "\n",
      "dinner.csv\n",
      "Counter({'b': 8191, 't': 5634, '2': 3056, '1': 2236, 'm': 430, 'c': 404, 'x': 323, 'p': 29})\n",
      "Counter({'b': 1462, 't': 922, '1': 557, '2': 198, 'm': 18, 'x': 13, 'p': 7})\n",
      "\n",
      "\n",
      "game.csv\n",
      "Counter({'1': 1905, 'b': 1544, '2': 896, 'm': 343, 't': 281, 'c': 62, 'x': 39})\n",
      "Counter({'b': 278, '1': 201, '2': 72, 'm': 24, 'x': 5})\n",
      "\n",
      "\n",
      "TV.csv\n",
      "Counter({'t': 2726, '2': 284, '1': 245, 'b': 202, 'c': 81, 'm': 29, 'x': 25})\n",
      "Counter({'t': 1428, '1': 35, 'b': 30})\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "每个场景的train/test标签对比情况\n",
    "\"\"\"\n",
    "for i in original_test_data:\n",
    "    print(i)\n",
    "    print(Counter(original_data[i][:,-1]))\n",
    "    print(Counter(original_test_data[i][:,-1]))\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "original_adaboost_dict = dict()\n",
    "for i in data:\n",
    "    rfc = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "    rfc.fit(original_data[i][:,:-1],original_data[i][:,-1])\n",
    "    original_adaboost_dict[i] = rfc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " outdoor.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.61      0.64       273\n",
      "           2       0.07      0.06      0.07       100\n",
      "           b       0.76      0.86      0.81       284\n",
      "           c       0.00      0.00      0.00         0\n",
      "           m       0.13      0.12      0.12        26\n",
      "           x       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.61       684\n",
      "   macro avg       0.27      0.27      0.27       684\n",
      "weighted avg       0.60      0.61      0.61       684\n",
      "\n",
      "\n",
      " reading.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.98      0.99      0.98       213\n",
      "           b       0.87      0.72      0.79        18\n",
      "\n",
      "    accuracy                           0.97       231\n",
      "   macro avg       0.92      0.86      0.89       231\n",
      "weighted avg       0.97      0.97      0.97       231\n",
      "\n",
      "\n",
      " call.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.89      0.98      0.93       336\n",
      "           b       0.00      0.00      0.00        23\n",
      "           m       0.00      0.00      0.00        23\n",
      "           p       0.80      0.87      0.83       125\n",
      "\n",
      "    accuracy                           0.86       507\n",
      "   macro avg       0.42      0.46      0.44       507\n",
      "weighted avg       0.78      0.86      0.82       507\n",
      "\n",
      "\n",
      " dinner.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.63      0.71       557\n",
      "           2       0.20      0.16      0.18       198\n",
      "           b       0.69      0.86      0.77      1462\n",
      "           c       0.00      0.00      0.00         0\n",
      "           m       0.05      0.11      0.07        18\n",
      "           p       0.00      0.00      0.00         7\n",
      "           t       0.64      0.50      0.56       922\n",
      "           x       0.00      0.00      0.00        13\n",
      "\n",
      "    accuracy                           0.66      3177\n",
      "   macro avg       0.30      0.28      0.28      3177\n",
      "weighted avg       0.66      0.66      0.65      3177\n",
      "\n",
      "\n",
      " game.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.55      0.32      0.41       201\n",
      "           2       0.58      0.31      0.40        72\n",
      "           b       0.85      0.78      0.81       278\n",
      "           c       0.00      0.00      0.00         0\n",
      "           m       0.09      0.46      0.15        24\n",
      "           t       0.00      0.00      0.00         0\n",
      "           x       0.00      0.00      0.00         5\n",
      "\n",
      "    accuracy                           0.54       580\n",
      "   macro avg       0.29      0.27      0.25       580\n",
      "weighted avg       0.67      0.54      0.59       580\n",
      "\n",
      "\n",
      " TV.csv\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           1       0.81      0.37      0.51        35\n",
      "           2       0.00      0.00      0.00         0\n",
      "           b       0.01      0.03      0.02        30\n",
      "           c       0.00      0.00      0.00         0\n",
      "           m       0.00      0.00      0.00         0\n",
      "           t       0.97      0.94      0.96      1428\n",
      "           x       0.00      0.00      0.00         0\n",
      "\n",
      "    accuracy                           0.91      1493\n",
      "   macro avg       0.26      0.19      0.21      1493\n",
      "weighted avg       0.95      0.91      0.93      1493\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in original_test_data:\n",
    "    temp_data = original_test_data[i]\n",
    "    print(\"\\n\",i)\n",
    "    y_pred = original_adaboost_dict[i].predict(temp_data[:,:-1]) \n",
    "    print(classification_report(original_test_data[i][:,-1],y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "这边开始是 cleanlab 洗标签过程\n",
    "[这部分我昨天做的时候是对6个场景分别训一个模型然后分别洗掉错误标签] 但过程里其实是有点小问题的，他这个算法原理需要做交叉验证然后从之前那个train/test label分布里面我们就看到在某些场景中\n",
    "其实有些label可能只有很少(甚至只有1个)。所以就意味着不能split,就会出现点bug\n",
    "\n",
    "[所以可能我觉得可以考虑不分场景先把数据洗一遍]\n",
    "\"\"\"\n",
    "\"\"\"\n",
    "以下代码都是昨天写的，只针对outdoor.csv这一个场景做了测试\n",
    "先把train和test各自洗一遍（实际情况是test由于某个label只有一个这个算法报错了，于是没洗test直接做的测试，结果也是很一般 '1'这个类基本区分不出来）\n",
    "\"\"\"\n",
    "from cleanlab.classification import LearningWithNoisyLabels\n",
    "temp_data = original_data['outdoor.csv']\n",
    "temp_test = original_test_data['outdoor.csv']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def temp_convert(value):\n",
    "    if value == '1':\n",
    "        return 0\n",
    "    elif value == '2':\n",
    "        return 1\n",
    "    elif value == 'b':\n",
    "        return 2\n",
    "    elif value == 'c':\n",
    "        return 3\n",
    "    elif value == 'm':\n",
    "        return 4\n",
    "    elif value == 'x':\n",
    "        return 5\n",
    "\n",
    "def temp_transform(data):\n",
    "    data_cp = data.copy()\n",
    "    for i in range(data.shape[0]):\n",
    "        temp = temp_convert(data[:,-1][i])\n",
    "        data_cp[:,-1][i] = temp\n",
    "    return data_cp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data = temp_transform(temp_data)\n",
    "temp_test = temp_transform(temp_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_data_X = np.array(temp_data[:,:-1],dtype=np.float)\n",
    "temp_data_Y = np.array(temp_data[:,-1],dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_test_X = np.array(temp_test[:,:-1],dtype=np.float)\n",
    "temp_test_Y = np.array(temp_test[:,-1],dtype=np.int64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 其实可以封装任意一个你自定义的模型.\n",
    "lnl = LearningWithNoisyLabels(clf=RandomForestClassifier(max_depth=30,random_state=0))\n",
    "lnl.fit(X=temp_data_X, s=temp_data_Y)\n",
    "# 对真实世界进行验证.\n",
    "predicted_test_labels = lnl.predict(temp_test_X)\n",
    "# print(classification_report(temp_test_Y,predicted_test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cleanlab\n",
    "psx = cleanlab.latent_estimation.estimate_cv_predicted_probabilities(\n",
    "    temp_data_X, temp_data_Y, clf=RandomForestClassifier(max_depth=30, random_state=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [],
   "source": [
    "from cleanlab.pruning import get_noise_indices\n",
    "psx = np.asarray(psx)\n",
    "ordered_label_errors = get_noise_indices(\n",
    "    s=temp_data_Y,\n",
    "    psx=psx,\n",
    "    sorted_index_method='normalized_margin', # Orders label errors\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_list_data = np.array(list(range(len(temp_data_X))))\n",
    "index_list_test = np.array(list(range(len(temp_test_X))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp_index_list_data = []\n",
    "for i in index_list_data:\n",
    "    if i in ordered_label_errors:\n",
    "        cp_index_list_data.append(False)\n",
    "    else:\n",
    "        cp_index_list_data.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=30, random_state=0)"
      ]
     },
     "execution_count": 260,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(max_depth=30, random_state=0)\n",
    "rfc.fit(temp_data_X[cp_index_list_data,:],temp_data_Y[cp_index_list_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/model_selection/_split.py:670: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn((\"The least populated class in y has only %d\"\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-276-fd86699078a6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0m再给test洗一次\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \"\"\"\n\u001b[0;32m----> 4\u001b[0;31m psx_test = cleanlab.latent_estimation.estimate_cv_predicted_probabilities(\n\u001b[0m\u001b[1;32m      5\u001b[0m     temp_test_X, temp_test_Y, clf=RandomForestClassifier(max_depth=30, random_state=0,min_samples_split=1))\n\u001b[1;32m      6\u001b[0m \u001b[0mpsx_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpsx_test\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/cleanlab/latent_estimation.py\u001b[0m in \u001b[0;36mestimate_cv_predicted_probabilities\u001b[0;34m(X, labels, clf, cv_n_folds, seed)\u001b[0m\n\u001b[1;32m    739\u001b[0m         should have been computed using 3 (or higher) fold cross-validation.\"\"\"\n\u001b[1;32m    740\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 741\u001b[0;31m     return estimate_py_noise_matrices_and_cv_pred_proba(\n\u001b[0m\u001b[1;32m    742\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    743\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/cleanlab/latent_estimation.py\u001b[0m in \u001b[0;36mestimate_py_noise_matrices_and_cv_pred_proba\u001b[0;34m(X, s, clf, cv_n_folds, thresholds, converge_latent_estimates, py_method, seed)\u001b[0m\n\u001b[1;32m    679\u001b[0m       joint count matrix i.e. confident joint, predicted probability matrix)\"\"\"\n\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 681\u001b[0;31m     confident_joint, psx = estimate_confident_joint_and_cv_pred_proba(\n\u001b[0m\u001b[1;32m    682\u001b[0m         \u001b[0mX\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    683\u001b[0m         \u001b[0ms\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/cleanlab/latent_estimation.py\u001b[0m in \u001b[0;36mestimate_confident_joint_and_cv_pred_proba\u001b[0;34m(X, s, clf, cv_n_folds, thresholds, seed, calibrate)\u001b[0m\n\u001b[1;32m    598\u001b[0m         \u001b[0;31m# Fit the clf classifier to the training set and\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    599\u001b[0m         \u001b[0;31m# predict on the holdout set and update psx.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 600\u001b[0;31m         \u001b[0mclf_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_cv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0ms_train_cv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    601\u001b[0m         \u001b[0mpsx_cv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf_copy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_holdout_cv\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# P(s = k|x) # [:,1]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    602\u001b[0m         \u001b[0mpsx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcv_holdout_idx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpsx_cv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    384\u001b[0m             \u001b[0;31m# parallel_backend contexts set at a higher level,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m             \u001b[0;31m# since correctness does not rely on using threads.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 386\u001b[0;31m             trees = Parallel(n_jobs=self.n_jobs, verbose=self.verbose,\n\u001b[0m\u001b[1;32m    387\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0m_joblib_parallel_args\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'threads'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    388\u001b[0m                 delayed(_parallel_build_trees)(\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1027\u001b[0m             \u001b[0;31m# remaining jobs.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1029\u001b[0;31m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1030\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[0;34m(self, iterator)\u001b[0m\n\u001b[1;32m    845\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    763\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    764\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 765\u001b[0;31m             \u001b[0mjob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    766\u001b[0m             \u001b[0;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    767\u001b[0m             \u001b[0;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[0;34m(self, func, callback)\u001b[0m\n\u001b[1;32m    206\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    207\u001b[0m         \u001b[0;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 208\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    209\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, batch)\u001b[0m\n\u001b[1;32m    570\u001b[0m         \u001b[0;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    571\u001b[0m         \u001b[0;31m# arguments in memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 572\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    573\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    574\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/joblib/parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    250\u001b[0m         \u001b[0;31m# change the default number of processes to -1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    251\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 252\u001b[0;31m             return [func(*args, **kwargs)\n\u001b[0m\u001b[1;32m    253\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/ensemble/_forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap)\u001b[0m\n\u001b[1;32m    166\u001b[0m                                                         indices=indices)\n\u001b[1;32m    167\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 168\u001b[0;31m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    169\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    170\u001b[0m         \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    888\u001b[0m         \"\"\"\n\u001b[1;32m    889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 890\u001b[0;31m         super().fit(\n\u001b[0m\u001b[1;32m    891\u001b[0m             \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    892\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.8/site-packages/sklearn/tree/_classes.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumbers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIntegral\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmin_samples_split\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m                 raise ValueError(\"min_samples_split must be an integer \"\n\u001b[0m\u001b[1;32m    229\u001b[0m                                  \u001b[0;34m\"greater than 1 or a float in (0.0, 1.0]; \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m                                  \u001b[0;34m\"got the integer %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: min_samples_split must be an integer greater than 1 or a float in (0.0, 1.0]; got the integer 1"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "再给test洗一次(注意这里就报错了，因为有某个label在这个test里只出现一次) \n",
    "\"\"\"\n",
    "psx_test = cleanlab.latent_estimation.estimate_cv_predicted_probabilities(\n",
    "    temp_test_X, temp_test_Y, clf=RandomForestClassifier(max_depth=30, random_state=0))\n",
    "psx_test = np.asarray(psx_test)\n",
    "ordered_label_errors_test = get_noise_indices(\n",
    "    s=temp_test_Y,\n",
    "    psx=psx_test,\n",
    "    sorted_index_method='normalized_margin', # Orders label errors\n",
    " )\n",
    "cp_index_list_test = []\n",
    "for i in index_list_test:\n",
    "    if i in ordered_label_errors_test:\n",
    "        cp_index_list_test.append(False)\n",
    "    else:\n",
    "        cp_index_list_test.append(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.73      0.71      0.72       273\n",
      "           1       0.26      0.35      0.30       100\n",
      "           2       0.87      0.87      0.87       284\n",
      "           4       0.00      0.00      0.00        26\n",
      "           5       0.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.69       684\n",
      "   macro avg       0.37      0.38      0.38       684\n",
      "weighted avg       0.69      0.69      0.69       684\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.8/site-packages/sklearn/metrics/_classification.py:1221: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "于是不洗直接测试\n",
    "可以参考这个链接里面有一个详细的置信学习的代码(一个具体例子)\n",
    "https://github.com/cgnorthcutt/cleanlab/blob/master/examples/simplifying_confident_learning_tutorial.ipynb\n",
    "\"\"\"\n",
    "temp_test_Y_pred = rfc.predict(temp_test_X)\n",
    "print(classification_report(temp_test_Y,temp_test_Y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 64-bit",
   "language": "python",
   "name": "python38564bit9755afc1b5134399b36eeb8f9d660dbb"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
